



## [大模型参数量和显存的换算关系](https://www.zhihu.com/question/612046818/answer/3438795176)

### **1B参数对应多少G显存？**

B和G都是十亿（1000M或1024M）的意思，M是100万的意思，平时说模型参数有**x**B就是说有**x**十亿个参数，平时说显存有多少G/M是说有多少G/M个**字节**（byte），1个字节=8比特（bit），那么，1B模型参数对应多少G内存和参数的精度有关，如果是全精度训练（fp32），一个参数对应32比特，也就是4个字节，参数换算到显存的时候要乘4，也就是1B模型参数对应4G显存，如果是fp16或者bf16就是乘2，1B模型参数对应2G显存。

### 有哪几部分显存开销，他们所占显存分别是参数的多少倍？

除了模型参数本身外，训练时的显存开销还有这几个部分：

- 梯度：一个参数对应一个梯度值，所以梯度所占显存是参数的1倍
- 优化器状态：取决于优化器的具体类型，如果是裸SGD就不需要额外显存开销，如果是带一阶动量（momentum）的SGD就是1倍，如果是Adam的话就要在momentum的基础上加上二阶动量，优化器状态所占显存就是参数的2倍。

小结：假设我们全参数微调训练一个参数量为1B的大模型，优化器为Adam，精度为fp32，忽略数据和hidden states部分的显存占用，那么显存占用为：参数的4G+梯度的4G+优化器状态的8G，共16G。如果是bf16精度训练则要减半，就是8G。如果是混合精度训练则根据各部分的精度调整计算过程。

## 并行

### 数据并行

![image-20231225104104256](assets/readme/image-20231225104104256.png)

### 模型并行

#### Tensor并行

在上面的这张图里，每一个节点（或者叫进程）都有一份模型，然后各个节点取不同的数据，通常是一个batch_size，然后各自完成前向和后向的计算得到梯度，这些进行训练的进程我们成为worker，除了worker，还有参数服务器，简称ps server，这些worker会把各自计算得到的梯度送到ps server，然后由ps server来进行update操作，然后把update后的模型再传回各个节点。因为在这种并行模式中，被划分的是数据，所以这种并行方式叫数据并行

![image-20231225104154125](assets/readme/image-20231225104154125.png)

深度学习的计算其实主要是矩阵运算，而在计算时这些矩阵都是保存在内存里的，如果是用GPU卡计算的话就是放在显存里，可是有的时候矩阵会非常大，比如在CNN中如果num_classes达到千万级别，那一个FC层用到的矩阵就可能会大到显存塞不下。这个时候就不得不把这样的超大矩阵给拆了分别放到不同的卡上去做计算，从网络的角度来说就是把网络结构拆了，其实从计算的过程来说就是把矩阵做了分块处理。

比较好理解，具体看Megatron论文，就是把一个神经网络层Tensor切成了多个小的Tensor，每个tensor放在不同的gpu。主要就是列并行、行并行。在transformer里的应用具体体现在MLP、Attention层里。

![image-20231225110603413](assets/readme/image-20231225110603413.png)



有的时候呢数据并行和模型并行会被同时用上。比如深度的卷积神经网络中卷积层计算量大，但所需参数系数 W 少，而FC层计算量小，所需参数系数 W 多。因此对于卷积层适合使用数据并行，对于全连接层适合使用模型并行。 就像这样

![image-20231225104358084](assets/readme/image-20231225104358084.png)

#### 流水线并行

流水并行是指按顺序将模型切分为不同的部分至不同的GPU上运行。每个GPU上只有部分参数，因此每个部分的模型消耗GPU的显存成比例减少。

将大型模型分为若干份连续的layer很简单。但是，layer的输入和输出之间存在顺序依赖关系，因此在一个GPU等待其前一个GPU的输出作为其输入时，朴素的实现会导致出现大量空闲时间。这些空闲时间被称作“气泡”，而在这些等待的过程中，空闲的机器本可以继续进行计算。

![image-20231225110835198](assets/readme/image-20231225110835198.png)

一个朴素的流水并行设置，其中模型按layer垂直分成 4 个部分。worker 1托管网络第一层（离输入最近）的模型参数，而 worker 4 托管第 4 层（离输出最近）的模型参数。“F”、“B”和“U”分别代表前向、反向和更新操作。下标指示数据在哪个节点上运行。由于顺序依赖性，数据一次只能在一个节点上运行。

了减少气泡的开销，在这里可以复用数据并行的打法，核心思想是将大批次数据分为若干个微批次数据（microbatches），每个节点每次只处理一个微批次数据，这样在原先等待的时间里可以进行新的计算。


每个微批次数据的处理速度会成比例地加快，每个节点在下一个小批次数据释放后就可以开始工作，从而加快流水执行。有了足够的微批次，节点大部分时间都在工作，而气泡在进程的开头和结束的时候最少。梯度是微批次数据梯度的平均值，并且只有在所有小批次完成后才会更新参数。

模型拆分的节点数通常被称为流水线深度（pipeline depth）。

在前向传递过程中，节点只需将其layer块的输出（激活）发送给下一个节点；在反向传递过程中，节点将这些激活的梯度发送给前一个节点。如何安排这些进程以及如何聚合微批次的梯度有很大的设计空间。GPipe 让每个节点连续前向和后向传递，在最后同步聚合多个微批次的梯度。PipeDream则是让每个节点交替进行前向和后向传递。

![image-20231225110938859](assets/readme/image-20231225110938859.png)

GPipe 和 PipeDream 流水方案对比。每批数据分为4个微批次，微批次1-8对应于两个连续大批次数据。图中，“（编号）”表示在哪个微批次上执行操作，下标表示节点 ID。其中，PipeDream使用相同的参数执行计算，可以获得更高的效率。



**参考：**

- https://www.zhihu.com/question/53851014
- [流水并行讲的好](https://www.zhihu.com/question/53851014/answer/2530594788)
- [图解大模型训练之：数据并行上篇(DP, DDP与ZeRO)](https://zhuanlan.zhihu.com/p/617133971)
- [图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)](https://zhuanlan.zhihu.com/p/618865052)








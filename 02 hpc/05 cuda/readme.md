 

# CUDA 编程

## 计算机架构

有多种不同的方法可以对计算机架构进行分类。一个广泛使用的分类方法是弗林分类法（Flynn’s Taxonomy），它根据指令和数据进入CPU的方式，将计算机架构分为4种不同的类型（如图1-6所示）。

![image-20240801085949080](assets/readme/image-20240801085949080.png)



- SISD指的是传统计算机：一种串行架构。在这种计算机上只有一个核心。在任何时间点上只有一个指令流在处理一个数据流。

- SIMD是一种并行架构类型。在这种计算机上有多个核心。在任何时间点上所有的核心只有一个指令流处理不同的数据流。向量机是一种典型的SIMD类型的计算机，现在大多数计算机都采用了SIMD架构。SIMD最大的优势或许就是，在CPU上编写代码时，程序员可以继续按串行逻辑思考但对并行数据操作实现并行加速，而其他细节则由编译器来负责。

- MISD类架构比较少见，在这种架构中，每个核心通过使用多个指令流处理同一个数据流。

- MIMD是一种并行架构，在这种架构中，多个核心使用多个指令流来异步处理多个数据流，从而实现空间上的并行性。许多MIMD架构还包括SIMD执行的子组件。

GPU代表了一种众核架构，几乎包括了前文描述的所有并行结构：多线程、MIMD（多指令多数据）、SIMD（单指令多数据），以及指令级并行。NVIDIA公司称这种架构为SIMT（单指令多线程）。

#### GPU核心和CPU核心

尽管可以使用多核和众核来区分CPU和GPU的架构，但这两种核心是完全不同的。

CPU核心比较重，用来处理非常复杂的控制逻辑，以优化串行程序执行。

GPU核心较轻，用于优化具有简单控制逻辑的数据并行任务，注重并行程序的吞吐量。

## cuda 编程模型

### 异构架构

![image-20240801090606975](assets/readme/image-20240801090606975.png)

一个典型的异构计算节点包括两个多核CPU插槽和两个或更多个的众核GPU。GPU不是一个独立运行的平台而是CPU的协处理器。因此，GPU必须通过PCIe总线与基于CPU的主机相连来进行操作，如图1-9所示。这就是为什么CPU所在的位置被称作主机端而GPU 所在的位置被称作设备端。

上图中的ALU指算术逻辑单元，它是CPU的一个重要组成部分。ALU负责执行各种算术和逻辑操作，包括加法、减法、乘法、除法、位运算等。

### 内存层次结构

在GPU内存层次结构中，最主要的两种内存是全局内存和共享内存。全局类似于CPU的系统内存，而共享内存类似于CPU的缓存。然而GPU的共享内存可以由CUDA C的内核直接控制

<img src="assets/readme/image-20240806151436569.png" alt="image-20240806151436569" style="zoom:50%;" />

### 线程管理

当核函数在主机端启动时，它的执行会移动到设备上，此时设备中会产生大量的线程并且每个线程都执行由核函数指定的语句。了解如何组织线程是CUDA编程的一个关键部分。CUDA明确了线程层次抽象的概念以便于你组织线程。这是一个两层的线程层次结构，由线程块和线程块网格构成，如图2-5所示。

<img src="assets/readme/image-20240806152148149.png" alt="image-20240806152148149" style="zoom:50%;" />



由一个内核启动所产生的所有线程统称为一个网格。同一网格中的所有线程共享相同的全局内存空间。一个网格由多个线程块构成，一个线程块包含一组线程，同一线程块内的线程可以通过共享内存的方式来协作。不同块内的线程不能协作。

线程依靠以下两个坐标变量来区分彼此。

- blockIdx（线程块在线程格内的索引）
- threadIdx（块内的线程索引）

### 函数类型限定符

函数类型限定符指定一个函数在主机上执行还是在设备上执行，以及可被主机调用还是被设备调用

![image-20240815181902730](assets/readme/image-20240815181902730.png)

#### CUDA核函数的限制

以下限制适用于所有核函数：

- 只能访问设备内存
- 必须具有void返回类型
- 不支持可变数量的参数
- 不支持静态变量
- 显示异步行为

## cuda 执行模型

## 内存布局

![image-20241226154917460](assets/readme/image-20241226154917460.png)

![image-20250102102828413](assets/readme/image-20250102102828413.png)

![image-20250102102814143](assets/readme/image-20250102102814143.png)

### 寄存器（Registers）

在 CUDA 编程中，局部变量默认存储在寄存器中（如果寄存器足够）。例如：

```cpp
__global__ void kernel(int *data) {
    int tid = threadIdx.x;  // tid 存储在寄存器中
    int temp = data[tid];   // temp 存储在寄存器中
    temp = temp * 2;        // 使用寄存器中的数据进行计算
    data[tid] = temp;       // 将结果写回全局内存
}
```

在上面的代码中：

- `tid` 和 `temp` 是局部变量，默认存储在寄存器中。
- 如果寄存器不足，编译器会将部分变量存储在本地内存

寄存器是 GPU 内存布局中最快、最接近计算核心的内存类型，用于存储线程的临时变量和中间计算结果。寄存器的访问速度极快，但容量有限。合理利用寄存器可以显著提高 GPU 程序的性能，而寄存器溢出（变量被存储在本地内存中）会导致性能下降。

寄存器溢出（**寄存器不足时的处理**）：如果线程使用的寄存器数量超过了硬件限制，编译器会将部分变量存储在 **本地内存（Local Memory）** 中。本地内存实际上是全局内存的一部分，访问速度比寄存器慢得多。

####  寄存器的容量

在 NVIDIA GPU 中，每个线程可以使用的寄存器数量是有限的，具体数量取决于 GPU 架构和编译器的配置。例如：

- **NVIDIA Tesla V100**（Volta 架构）：每个线程最多可以使用 **255 个寄存器**。
- **NVIDIA A100**（Ampere 架构）：每个线程最多可以使用 **255 个寄存器**。
- 较旧的架构（如 Kepler、Maxwell）可能支持更少的寄存器。

#### **寄存器的大小**

在 GPU 和大多数现代计算机体系结构中，**一个寄存器通常是 4 字节（32 位）**，这是由硬件设计、历史演变和应用需求共同决定的。以下是原因：

- **数据对齐和效率**

  - **4 字节对齐**：现代计算机体系结构通常以 4 字节为单位进行数据对齐，这样可以提高内存访问效率。

  - **硬件优化**：32 位寄存器的设计可以高效地处理整数、浮点数和其他常见数据类型，同时简化硬件电路的设计。

- **指令集架构**

  - **32 位指令集**：许多现代处理器（如 x86、ARM、NVIDIA GPU）使用 32 位指令集，寄存器的大小与指令集匹配，可以简化指令解码和执行。

  - **通用性**：32 位寄存器可以处理 8 位（char）、16 位（short）、32 位（int）和 64 位（long long）数据类型，具有较好的通用性

### 本地内存（Local Memory）

本地内存是存储在每个线程本地的内存，通常由编译器自动分配。当寄存器不足时，一些变量可能被存储在本地内存中。

“本地内存”这一名词是有歧义的：溢出到本地内存中的变量本质上与全局内存在同一块存储区域，因此本地内存访问的特点是高延迟和低带宽。

### 共享内存（Shared Memory）

共享内存是GPU中一种比全局内存访问速度更快的内存类型。它位于GPU的多个处理单元之间共享，主要用于加速线程之间的通信和协作。在CUDA中，程序员可以使用`__shared__`关键字定义共享内存。

### 常量内存（Constant Memory）

常量内存用于存储在GPU上不变的数据，例如常量参数或只读数据。常量内存通常有更高的缓存效果，但写入操作是禁止的。在CUDA中，可以使用`__constant__`关键字定义常量内存。

### 纹理内存Texture Memory）

纹理内存驻留在设备内存中，并在每个SM的只读缓存中缓存。纹理内存是一种通过指定的只读缓存访问的全局内存。只读缓存包括硬件滤波的支持，它可以将浮点插入作为读过程的一部分来执行。纹理内存是对二维空间局部性的优化，所以线程束里使用纹理内存访问二维数据的线程可以达到最优性能。对于一些应用程序来说，这是理想的内存，并由于缓存和滤波硬件的支持所以有较好的性能优势。然而对于另一些应用程序来说，与全局内存相比，使用纹理内存更慢。

纹理内存用于存储2D图像数据，通常用于图形处理和一些科学计算应用。纹理内存支持一些特殊的访问模式，以提高图像处理的效率。

### 全局内存（Global Memory）

全局内存是GPU上最大的内存池，用于存储持久性数据。它通常位于设备内存中，与主机内存相对应。全局内存在设备之间共享，但访问速度相对较慢。在CUDA编程中，全局内存通过`cudaMalloc`和`cudaMemcpy`等函数进行分配和传输。

## GPU缓存

跟CPU缓存一样，GPU缓存是不可编程的内存。在GPU上有4种缓存：

- 一级缓存

- 二级缓存

- 只读常量缓存

- 只读纹理缓存

每个SM都有一个一级缓存，所有的SM共享一个二级缓存。一级和二级缓存都被用来在存储本地内存和全局内存中的数据，也包括寄存器溢出的部分。对Fermi GPU和KeplerK40或其后发布的GPU来说，CUDA允许我们配置读操作的数据是使用一级和二级缓存，还是只使用二级缓存。

在CPU上，内存的加载和存储都可以被缓存。但是，在GPU上只有内存加载操作可以被缓存，内存存储操作不能被缓存。

每个SM也有一个只读常量缓存和只读纹理缓存，它们用于在设备内存中提高来自于各自内存空间内的读取性能。



## 流和并发
